{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import (\n",
    "    DataLoader\n",
    ")\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    GenerationConfig,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_collator =DataCollatorWithPadding(tokenizer=reward_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    load_accuracy = load_metric(\"accuracy\")\n",
    "    load_f1 = load_metric(\"f1\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "   output_dir=\"new_reward/\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=2,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   push_to_hub=True,\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "   model=reward_model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=reward_tokenizer,\n",
    "   data_collator=reward_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404f9ea1954246979b92416af3fdc24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 242.9132, 'train_samples_per_second': 24.7, 'train_steps_per_second': 1.548, 'train_loss': 0.3249765355536278, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/1d/79/1d79f7e31a600f4e7c912f958a51a54fa60bc084de005be55cec53d4f66c2851/6157b0a47e246c73c596e36e86a5e83c1485d9d7b85d7be19f61515817d70994?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20240805%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240805T164212Z&X-Amz-Expires=86400&X-Amz-Signature=46b5097d3f3c99ce48223698147372728fe0f986b628998cb8e7823edc1e9d63&X-Amz-SignedHeaders=host&partNumber=11&uploadId=wNAQfXwI6IRtKWc_6djEe31C7GITVN6dqCPeMW.AO2E6XqdIX_yOzxg8i9.K6kWvczIQAWDIewkarNGp65oBhcLPZhFhaAV9AtTHZeU6Yz2jgTMgSRlm6gEDfLJtH7wb&x-id=UploadPart (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2427)')))\"), '(Request ID: eeefbb8f-e8b5-4baa-aac1-fd8378e8c5a0)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/1d/79/1d79f7e31a600f4e7c912f958a51a54fa60bc084de005be55cec53d4f66c2851/6157b0a47e246c73c596e36e86a5e83c1485d9d7b85d7be19f61515817d70994?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20240805%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240805T164212Z&X-Amz-Expires=86400&X-Amz-Signature=46b5097d3f3c99ce48223698147372728fe0f986b628998cb8e7823edc1e9d63&X-Amz-SignedHeaders=host&partNumber=11&uploadId=wNAQfXwI6IRtKWc_6djEe31C7GITVN6dqCPeMW.AO2E6XqdIX_yOzxg8i9.K6kWvczIQAWDIewkarNGp65oBhcLPZhFhaAV9AtTHZeU6Yz2jgTMgSRlm6gEDfLJtH7wb&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=376, training_loss=0.3249765355536278, metrics={'train_runtime': 242.9132, 'train_samples_per_second': 24.7, 'train_steps_per_second': 1.548, 'total_flos': 784349816457696.0, 'train_loss': 0.3249765355536278, 'epoch': 2.0})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86708bd5734a4170be304cd7b1a39bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/rz9pfrnd34q32tv5679ygvyh0000gn/T/ipykernel_30537/3234682954.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  load_accuracy = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3420860171318054,\n",
       " 'eval_accuracy': 0.8833333333333333,\n",
       " 'eval_f1': 0.882154882154882,\n",
       " 'eval_runtime': 6.0713,\n",
       " 'eval_samples_per_second': 49.413,\n",
       " 'eval_steps_per_second': 3.129,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier = pipeline(task = 'sentiment-analysis', model=reward_model, tokenizer=reward_tokenizer, device=\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARP Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model_name = \"lvwerra/gpt2-imdb\"\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(sft_model_name)\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(sft_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since imdb couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /Users/ilyamikheev/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31 (last modified on Mon Aug  5 01:27:39 2024).\n"
     ]
    }
   ],
   "source": [
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_tokenizer.pad_token = sft_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e001edb62e4532b9841eb36eee3849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = imdb['train'].map(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_tokenizer.pad_token = sft_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_train_dataset = imdb['train'].shuffle(seed=42).select([i for i in list(range(3000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_generator():\n",
    "    while True:\n",
    "        yield from reduced_train_dataloader\n",
    "\n",
    "iter_dataloader = iter(repeat_generator())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d0b29b41a34c6da6be15ef54b4d961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   40,   655,   836,  ...,  2138,   621,   340],\n",
      "        [   40,  6151,   428,  ...,   530,  2665,   286],\n",
      "        [ 5779,    11,   314,  ...,   976,    11,   393],\n",
      "        ...,\n",
      "        [   32,   288,   260,  ...,   876,    13, 50256],\n",
      "        [ 1722,   257,  9887,  ..., 10892,  6490,   737],\n",
      "        [   40,  7342,   428,  ...,   379,   257,   922]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_truncate(item, tokenizer, max_length):\n",
    "    text = item['text']\n",
    "    tokens = tokenizer(text, truncation=True, padding='max_length', max_length=max_length)\n",
    "    return tokens\n",
    "\n",
    "max_length = 53\n",
    "tokenized_dataset = reduced_train_dataset.map(lambda x: tokenize_and_truncate(x, sft_tokenizer, max_length), batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_generator():\n",
    "    while True:\n",
    "        yield from dataloader\n",
    "\n",
    "iter_dataloader = iter(repeat_generator())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1212,  4947,   286,  ...,   511,  3164,   284],\n",
       "         [ 1273,  6582,   647,  ...,  9640,    13,   314],\n",
       "         [ 7149,  2646,   287,  ...,  7328,   588,   376],\n",
       "         ...,\n",
       "         [ 1026,   338,   407,  ...,   356,  6004,   284],\n",
       "         [  464,  3159, 16002,  ...,   852,  1223,  2041],\n",
       "         [   40,  1107,  3114,  ...,  1671,  1220,  6927]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[5661,  318,  262,  ..., 1521,  616, 3632],\n",
      "        [  72,  460,  470,  ...,  318,  326,  340],\n",
      "        [  40, 4724,  428,  ...,   27, 1671, 1220],\n",
      "        ...,\n",
      "        [3152,  326, 1627,  ..., 9563, 8470,  290],\n",
      "        [2953,  530,  886,  ..., 5545,  416,  281],\n",
      "        [  40, 1842,  428,  ...,  883, 1528,   13]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in iter_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
